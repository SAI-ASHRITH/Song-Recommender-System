{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CMPE 256 Programming Assignment\n",
    "### Name: Sai Ashrith Aduwala\n",
    "### SJSU Id: 014427725\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import *\n",
    "import pandas as pd\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection.split import train_test_split\n",
    "from surprise.model_selection import cross_validate, GridSearchCV\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the train and test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Track</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User  Track  Rating\n",
       "0     1      0      27\n",
       "1     4      0      10\n",
       "2     8      0      30\n",
       "3    10      0      30\n",
       "4    11      0      14"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv('train.csv')         #Load the training data.\n",
    "tt = pd.read_csv('test.csv')         #Load the testing data.\n",
    "#tdf = pd.read_csv('newtrain.csv')   #Load the modified training data.\n",
    "df.head(5)                           #Display the first five rows of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metadata about the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Track</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150999.000000</td>\n",
       "      <td>150999.000000</td>\n",
       "      <td>150999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>26512.487997</td>\n",
       "      <td>86.650263</td>\n",
       "      <td>36.433274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13633.371921</td>\n",
       "      <td>56.020086</td>\n",
       "      <td>22.582832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>17735.500000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>27878.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>35966.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>50927.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                User          Track         Rating\n",
       "count  150999.000000  150999.000000  150999.000000\n",
       "mean    26512.487997      86.650263      36.433274\n",
       "std     13633.371921      56.020086      22.582832\n",
       "min         0.000000       0.000000       0.000000\n",
       "25%     17735.500000      36.000000      15.000000\n",
       "50%     27878.000000      81.000000      32.000000\n",
       "75%     35966.000000     142.000000      50.000000\n",
       "max     50927.000000     183.000000     100.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for any null values in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "User      0\n",
       "Track     0\n",
       "Rating    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the reader variable with a scale of 0-100.\n",
    "\n",
    "Transforming the training data from a dataframe format to a Reader format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(0,100))\n",
    "data=Dataset.load_from_df(df,reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the train data into training set and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(data, test_size=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating variables for different algorithms in the surprise library using their default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "blo = BaselineOnly()   #surprise.prediction_algorithms.baseline_only.BaselineOnly(bsl_options={}, verbose=True)\n",
    "\n",
    "sim_options={\n",
    "    'user_based':False    #For this problem we need item based model, thus, this value is set to false.\n",
    "}\n",
    "\n",
    "knnb = KNNBasic(sim_options=sim_options)   #surprise.prediction_algorithms.knns.KNNBasic(k=40, min_k=1, sim_options={}, verbose=True, **kwargs)\n",
    "\n",
    "knnm = KNNWithMeans(sim_options=sim_options)   #surprise.prediction_algorithms.knns.KNNWithMeans(k=40, min_k=1, sim_options={}, verbose=True, **kwargs)\n",
    "\n",
    "knnz = KNNWithZScore(sim_options=sim_options)   #surprise.prediction_algorithms.knns.KNNWithZScore(k=40, min_k=1, sim_options={}, verbose=True, **kwargs)\n",
    "\n",
    "knnl = KNNBaseline(sim_options=sim_options)   #surprise.prediction_algorithms.knns.KNNBaseline(k=40, min_k=1, sim_options={}, bsl_options={}, verbose=True, **kwargs)\n",
    "\n",
    "svd = SVD()   #surprise.prediction_algorithms.matrix_factorization.SVD\n",
    "\n",
    "so = SlopeOne()   #surprise.prediction_algorithms.slope_one.SlopeOne\n",
    "\n",
    "cc = CoClustering()   #surprise.prediction_algorithms.co_clustering.CoClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the models with the training set(train_df) and testing them with the test set(test_df)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    }
   ],
   "source": [
    "blo.fit(train_df)                           #BaselineOnly()\n",
    "pred_blo = blo.test(test_df)\n",
    "\n",
    "knnb.fit(train_df)                          #KNNBasic()\n",
    "pred_knnb = knnb.test(test_df)\n",
    "\n",
    "knnm.fit(train_df)                          #KNNWithMeans()\n",
    "pred_knnm = knnm.test(test_df)\n",
    "\n",
    "knnz.fit(train_df)                          #KNNWithZScore()\n",
    "pred_knnz = knnz.test(test_df)\n",
    "\n",
    "knnl.fit(train_df)                          #KNNBaseline()\n",
    "pred_knnl = knnl.test(test_df)\n",
    "\n",
    "svd.fit(train_df)                           #SVD()\n",
    "pred_svd = svd.test(test_df)\n",
    "\n",
    "so.fit(train_df)                            #SlopeOne()\n",
    "pred_so = so.test(test_df)\n",
    "\n",
    "cc.fit(train_df)                            #CoClustering()\n",
    "pred_cc = cc.test(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the RMSE scores for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaselineOnly\n",
      "RMSE: 19.6346\n",
      "\n",
      "\n",
      "KNNBasic\n",
      "RMSE: 18.8269\n",
      "\n",
      "\n",
      "KNNWithMeans\n",
      "RMSE: 17.6878\n",
      "\n",
      "\n",
      "KNNWithZScore\n",
      "RMSE: 17.6097\n",
      "\n",
      "\n",
      "KNNBaseline\n",
      "RMSE: 17.5664\n",
      "\n",
      "\n",
      "SVD\n",
      "RMSE: 16.2095\n",
      "\n",
      "\n",
      "SlopeOne\n",
      "RMSE: 18.5940\n",
      "\n",
      "\n",
      "CoClustering\n",
      "RMSE: 19.6638\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_list = [pred_blo, pred_knnb, pred_knnm, pred_knnz, pred_knnl, pred_svd, pred_so, pred_cc]   #list of predictions\n",
    "model_list = [\"BaselineOnly\",\"KNNBasic\",\"KNNWithMeans\",\"KNNWithZScore\",\"KNNBaseline\",\"SVD\",\"SlopeOne\",\"CoClustering\"]  #list of algorithms\n",
    "x=0;\n",
    "for i in pred_list:\n",
    "    print(model_list[x])                    #Print the model name.\n",
    "    x+=1\n",
    "    accuracy.rmse(i, verbose=True)          #Print the RMSE score for that model.\n",
    "    print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm with the least RMSE score is SVD. Therefore choose SVD for offline and hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_algo = SVD(n_factors=400, reg_all=0.0001, n_epochs=30)    #Perform offline parameter tuning by manually trying and changing the parameters for the algorithm.\n",
    "svd_algo.fit(train_df)\n",
    "prediction = svd_algo.test(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE score after performing offline parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 16.5273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16.527291436653663"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.rmse(prediction,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    16.8585 16.7417 16.7627 16.7739 16.7961 16.7866 0.0400  \n",
      "Fit time          19.48   18.95   18.83   19.18   18.86   19.06   0.24    \n",
      "Test time         0.25    0.25    0.24    0.24    0.26    0.25    0.01    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_rmse': array([16.85845613, 16.74171125, 16.762664  , 16.77386217, 16.79614709]),\n",
       " 'fit_time': (19.479861736297607,\n",
       "  18.952356576919556,\n",
       "  18.828615188598633,\n",
       "  19.184714555740356,\n",
       "  18.864559650421143),\n",
       " 'test_time': (0.2543203830718994,\n",
       "  0.24733924865722656,\n",
       "  0.24235105514526367,\n",
       "  0.2403578758239746,\n",
       "  0.26427245140075684)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_cv = SVD(n_factors=300, reg_all = 0.0001)\n",
    "cross_validate(svd_cv, data, measures=['RMSE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing hyperparameter tuning using GridSearchCV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_factors': [300, 400],           #The set of parameters used for model tuning.\n",
    "              'reg_all': [0.0001, 0.001],\n",
    "              'n_epochs': [20, 30]\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(SVD, parameters, measures=['rmse'], cv=5)  #Initialize the GridSearchCV() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(data)   #Build and test the model using the different combinations of the above given parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.77208423326956\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_score['rmse'])    #Print the best RMSE score achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_factors': 300, 'reg_all': 0.0001, 'n_epochs': 30}\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_params['rmse'])   #Print the parameters for which we got the best RMSE score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x24e9efb8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_grid = grid.best_estimator['rmse']   #Using the best parameters rebuild the model using the full trainset.\n",
    "svd_grid.fit(data.build_full_trainset())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the rating for the users in the test data read from the 'test.csv' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('predictions.csv', 'w')         #Create/open a file to store the predictions\n",
    "header=\"Id,Rating\"                          #Define the headers for the prediction file\n",
    "file.write(header+\"\\n\")                     #Insert the headers to the file\n",
    "for i in range(0, len(tt)):\n",
    "    predicted_rating = 0.0\n",
    "    output = None\n",
    "    pred=svd_grid.predict(uid=tt.iloc[i]['User'],iid=tt.iloc[i]['Track'])        #Predict the rating for each user in the test data by giving the 'User' and 'Track' cloumns as the parameters.\n",
    "    output =str(pred[0]) + \"-\" + str(pred[1]) + \",\"                          #Format the 'User' and 'Track' numbers and store in 'output' variable.\n",
    "    predicted_rating = predicted_rating + float(pred[3])                     #Update the predicted rating for the above user and track.\n",
    "    output = output + str(predicted_rating)                                  #Store the predicted rating in the 'output' variable.\n",
    "    file.write(output + \"\\n\")                                                #Write the value of the 'output' variable to the 'predictions.csv' file.\n",
    "    \n",
    "file.close()                                #Close the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
